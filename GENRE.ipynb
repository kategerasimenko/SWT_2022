{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpyUXp5a86Oo"
      },
      "source": [
        "## Installation and downloads\n",
        "original debugging code is [here](https://colab.research.google.com/drive/1Lx9pIxX0JYOGFG0Aoe39qW4N5vmOV74G?usp=sharing).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed0HCRYTzU1x",
        "outputId": "e7a16d9f-e630-4b6b-fe45-c2809e85b9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-12 10:41:54--  https://dl.fbaipublicfiles.com/GENRE/fairseq_multilingual_entity_disambiguation.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2057741836 (1.9G) [application/gzip]\n",
            "Saving to: ‘fairseq_multilingual_entity_disambiguation.tar.gz’\n",
            "\n",
            "fairseq_multilingua 100%[===================>]   1.92G  24.1MB/s    in 84s     \n",
            "\n",
            "2022-11-12 10:43:19 (23.4 MB/s) - ‘fairseq_multilingual_entity_disambiguation.tar.gz’ saved [2057741836/2057741836]\n",
            "\n",
            "fairseq_multilingual_entity_disambiguation/\n",
            "fairseq_multilingual_entity_disambiguation/model.pt\n",
            "fairseq_multilingual_entity_disambiguation/dict.source.txt\n",
            "fairseq_multilingual_entity_disambiguation/dict.target.txt\n",
            "fairseq_multilingual_entity_disambiguation/spm_256000.model\n",
            "fairseq_multilingual_entity_disambiguation/spm_256000.vocab\n",
            "--2022-11-12 10:44:03--  http://dl.fbaipublicfiles.com/GENRE/titles_lang_all105_marisa_trie_with_redirect.pkl\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.75.142, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 581590386 (555M) [application/octet-stream]\n",
            "Saving to: ‘titles_lang_all105_marisa_trie_with_redirect.pkl’\n",
            "\n",
            "titles_lang_all105_ 100%[===================>] 554.65M  25.9MB/s    in 23s     \n",
            "\n",
            "2022-11-12 10:44:27 (24.1 MB/s) - ‘titles_lang_all105_marisa_trie_with_redirect.pkl’ saved [581590386/581590386]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://dl.fbaipublicfiles.com/GENRE/fairseq_multilingual_entity_disambiguation.tar.gz\n",
        "! tar -xvf fairseq_multilingual_entity_disambiguation.tar.gz\n",
        "\n",
        "! wget http://dl.fbaipublicfiles.com/GENRE/titles_lang_all105_marisa_trie_with_redirect.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgImLyR_x0hr",
        "outputId": "a57a166a-3a9d-49e0-a314-87a7ccc34952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 25465, done.\u001b[K\n",
            "remote: Total 25465 (delta 0), reused 0 (delta 0), pack-reused 25465\u001b[K\n",
            "Receiving objects: 100% (25465/25465), 19.82 MiB | 18.90 MiB/s, done.\n",
            "Resolving deltas: 100% (18491/18491), done.\n"
          ]
        }
      ],
      "source": [
        "# remove locally installed `examples` package to avoid import error\n",
        "! rm -rf /usr/local/lib/python3.7/dist-packages/examples\n",
        "\n",
        "! git clone --branch fixing_prefix_allowed_tokens_fn https://github.com/nicola-decao/fairseq\n",
        "\n",
        "# remove the bugged lines in this fairseq version\n",
        "! sed -i -e '26,27d' /content/fairseq/fairseq/registry.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPqPsJaZYdZ1",
        "outputId": "cede4ce1-6c6f-441f-dd8f-16945635b98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/fairseq\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+4c4d5a7) (4.64.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+4c4d5a7) (1.15.1)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 14.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+4c4d5a7) (2022.6.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+4c4d5a7) (1.12.1+cu113)\n",
            "Collecting hydra-core<1.1\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+4c4d5a7) (0.29.32)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+4c4d5a7) (1.21.6)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (5.10.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+4c4d5a7) (4.1.1)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+4c4d5a7) (4.9.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+4c4d5a7) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1->fairseq==1.0.0a0+4c4d5a7) (3.10.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+4c4d5a7-cp37-cp37m-linux_x86_64.whl size=3003283 sha256=eca5005407318dca21045c98f53193d4f28c8b92e7a41065593ed70edce83739\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-se8pbn1f/wheels/7c/35/80/edbd520a1a7e615df007002aeea9f6bf5f3c8f9243e072f6ce\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=441c18ee7247bbb25a9c91c710c796d6752aff5fde82e20203400f73eaabdee7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 colorama-0.4.6 fairseq-1.0.0a0+4c4d5a7 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.6.0 sacrebleu-2.3.1\n"
          ]
        }
      ],
      "source": [
        "! cd fairseq && pip install  ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hb--qs5OAx2n",
        "outputId": "3ca14f9e-28a0-4f6b-b0b9-5c8b4b50ef88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "examples\n"
          ]
        }
      ],
      "source": [
        "# further path fixes\n",
        "\n",
        "! mkdir -p examples_dir\n",
        "! cp -r /content/fairseq/examples/ /content/examples_dir\n",
        "! ls /content/examples_dir\n",
        "\n",
        "! cp -r /content/fairseq/fairseq/models/speech_to_text/modules /usr/local/lib/python3.7/dist-packages/fairseq/models/speech_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPtRnBalzLGW",
        "outputId": "fad4f3e3-9fc0-4f18-c70d-b52b0cc705ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GENRE'...\n",
            "remote: Enumerating objects: 457, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 457 (delta 111), reused 148 (delta 99), pack-reused 284\u001b[K\n",
            "Receiving objects: 100% (457/457), 11.00 MiB | 15.20 MiB/s, done.\n",
            "Resolving deltas: 100% (258/258), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf GENRE\n",
        "!git clone https://github.com/facebookresearch/GENRE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3sSRRHe8lWD",
        "outputId": "7d4a95a2-735a-4a98-d8e6-37c64d139663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/GENRE\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: genre\n",
            "  Building wheel for genre (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for genre: filename=genre-0.1.3-py3-none-any.whl size=22505 sha256=ccb0941dfe6bfa09d317352b2bbb03a1c0c94e4308bce06ef16d0869bfdb7066\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cl5i5zx6/wheels/b2/27/2f/b6150a2b3523fe20a649241903adbdad72647075980b130654\n",
            "Successfully built genre\n",
            "Installing collected packages: genre\n",
            "Successfully installed genre-0.1.3\n"
          ]
        }
      ],
      "source": [
        "! cd GENRE && pip install ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSkPf1xQ0p0k",
        "outputId": "8d5569e8-a6c0-4c31-fe34-f8f5cd51116e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 15.5 MB/s \n",
            "\u001b[?25hCollecting marisa_trie\n",
            "  Downloading marisa_trie-0.7.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from marisa_trie) (57.4.0)\n",
            "Installing collected packages: sentencepiece, marisa-trie\n",
            "Successfully installed marisa-trie-0.7.8 sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "! pip install sentencepiece marisa_trie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUw14XCQIlRE",
        "outputId": "45b39d94-2701-4cef-b27f-68ca9936af57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting es-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9 MB 103.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ru-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.4.0/ru_core_news_sm-3.4.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3 MB 15.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from ru-core-news-sm==3.4.0) (3.4.2)\n",
            "Collecting pymorphy2>=0.9\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 31.7 MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->ru-core-news-sm==3.4.0) (2.0.1)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=3e5b4f591d98ded29db6c12f460476fc7b5e0f1666c9559fa577f03ffc23a73f\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, ru-core-news-sm\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 ru-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# ! pip install spacy\n",
        "! python -m spacy download es_core_news_sm\n",
        "! python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbcckrQ9NKb"
      },
      "source": [
        "# Parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ5Hbgb3LDQN",
        "outputId": "158bf61f-b848-4cf8-edcb-6e27d33280eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CnFypT0oa0RO"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/SWT/final/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cdEUlzWbBU-O"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/GENRE/genre')\n",
        "sys.path.append('/content/examples_dir')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8HX-jWO6zyOF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from time import sleep\n",
        "\n",
        "import spacy\n",
        "import requests\n",
        "from lxml import etree\n",
        "\n",
        "from fairseq_model import mGENRE\n",
        "from genre.trie import Trie, MarisaTrie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fn4QSkyacGUM"
      },
      "outputs": [],
      "source": [
        "NAMESPACE = {'ns': 'http://www.tei-c.org/ns/1.0'}\n",
        "UTTERANCE_XPATH = '//ns:sp'\n",
        "TEXT_XPATH = './ns:p'\n",
        "LOC_XPATH = './/ns:loc'\n",
        "\n",
        "SPACE_REGEX = re.compile(r'\\s+')\n",
        "\n",
        "MAX_CONTEXT_CHARS = 1000  # context limit from one side of location\n",
        "\n",
        "REQUEST_URL = 'https://%s.wikipedia.org/w/api.php'\n",
        "USER_AGENT = {'User-Agent': 'Location extractor (e.garanina@student.rug.nl)'}\n",
        "N_TITLES_PER_REQUEST = 50\n",
        "\n",
        "TITLE_REGEX = re.compile(r'^(.+?) >> (.+)$')\n",
        "GENRE_THRESHOLD = -0.65\n",
        "\n",
        "WIKILANGS = {'ru', 'es'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LrPfYusC2bxH"
      },
      "outputs": [],
      "source": [
        "GENRE_MODEL = mGENRE.from_pretrained(\"fairseq_multilingual_entity_disambiguation\").eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HwBmFYE31JxO"
      },
      "outputs": [],
      "source": [
        "# memory efficient but slower prefix tree (trie) -- it is implemented with `marisa_trie`\n",
        "with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n",
        "    TRIE = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RkoLd6mVI8Sx"
      },
      "outputs": [],
      "source": [
        "SPACY_OBJS = {\n",
        "    'span': spacy.load(\"es_core_news_sm\"),\n",
        "    'rus': spacy.load(\"ru_core_news_sm\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VZvvqFAOJwQu"
      },
      "outputs": [],
      "source": [
        "# Cut context of the location so that it fits into the model.\n",
        "\n",
        "def sentence_split(lang, text):\n",
        "    \"\"\"\n",
        "    Split text into sentences.\n",
        "    Return sentences and their indices in the text.\n",
        "    \"\"\"\n",
        "    # workaround to avoid splitting by [START] or [END]\n",
        "    clear_text = text.replace('[START] ', 'a' * 8).replace(' [END]', 'a' * 6)\n",
        "\n",
        "    nlp = SPACY_OBJS[lang]\n",
        "    doc = nlp(clear_text)\n",
        "    assert doc.has_annotation(\"SENT_START\")\n",
        "  \n",
        "    sentence_idxs = [(s.start_char, s.end_char) for s in doc.sents]\n",
        "    sentences = [text[s:e] for s, e in sentence_idxs]\n",
        "  \n",
        "    return sentences, sentence_idxs\n",
        "\n",
        "\n",
        "def get_location_sentence_id(loc_start, loc_end, sentence_idxs):\n",
        "    \"\"\"\n",
        "    Get sentence idx of the parsed location\n",
        "    based on indices of location and sentences in the text.\n",
        "    \"\"\"\n",
        "    loc_sentence_id = 0\n",
        "    n_sentences = len(sentence_idxs)\n",
        "\n",
        "    # iterate until start of location is in the current sentence\n",
        "    while loc_start >= sentence_idxs[loc_sentence_id][1]:\n",
        "        loc_sentence_id += 1\n",
        "\n",
        "    # print(loc_start, loc_end, sentence_idxs)\n",
        "    # end of location must be in the same sentence\n",
        "    assert loc_end < sentence_idxs[loc_sentence_id][1]\n",
        "    return loc_sentence_id\n",
        "\n",
        "\n",
        "def find_context(sentences, loc_sentence_id, loc_span, loc_sentence_start):\n",
        "    \"\"\"\n",
        "    Limit context for locations \n",
        "    (due to sequence length restriction in the model).\n",
        "    Take sentences from left and right until \n",
        "    the context length reaches MAX_CONTEXT_CHARS.\n",
        "    \"\"\"\n",
        "    context = {\n",
        "        'left': {\n",
        "            'offset': 0,  # absolute diff between current sentence idx and loc_sentence_id\n",
        "            'n_chars': 0,  # number of characters in already taken sentences\n",
        "            'increment': -1,  # \"direction\" of incrementing the indices from loc_sentence_id\n",
        "            'start': loc_span[0] - loc_sentence_start  # start idx of location inside loc_sentence_id\n",
        "        },\n",
        "        'right': {\n",
        "            'offset': 0,\n",
        "            'n_chars': 0,\n",
        "            'increment': 1,\n",
        "            'start': loc_span[1] - loc_sentence_start\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # consider right and left context separately\n",
        "    for key, info in context.items():\n",
        "        idx = loc_sentence_id\n",
        "        stable_idx = loc_sentence_id\n",
        "\n",
        "        # take new sentences until n_chars exceeds limit\n",
        "        while info['n_chars'] < MAX_CONTEXT_CHARS:\n",
        "            stable_idx = idx  # previous idx which did not exceed the limit\n",
        "\n",
        "            # if we consider sentence with loc (at first step), \n",
        "            # add to n_chars n_symbols before / after the location\n",
        "            if info['offset'] == 0:\n",
        "                info['n_chars'] += len(sentences[loc_sentence_id][:info['start']])\n",
        "            else:\n",
        "                # new index ty try\n",
        "                idx = loc_sentence_id + (info['offset'] * info['increment'])\n",
        "\n",
        "                # if new idx is out of range, end the loop\n",
        "                if idx < 0 or idx == len(sentences):\n",
        "                  break\n",
        "\n",
        "                info['n_chars'] += len(sentences[idx])\n",
        "\n",
        "            # increasing the diff (i.e. going one sent further)\n",
        "            info['offset'] += 1\n",
        "\n",
        "        # if we're out of the loop, \n",
        "        # info['idx'] is out of the range or limit of n_chars is exceeded, \n",
        "        # so take idx from previous iteration, which passed\n",
        "        info['idx'] = stable_idx\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def cut_context(text, lang):\n",
        "    \"\"\" \n",
        "    Cut sentences from left and right context of location.\n",
        "    Split text into sentences with spacy, \n",
        "    cut sentences which do not fit into MAX_CONTEXT_CHARS limit.\n",
        "    \"\"\"\n",
        "    # if the whole text is much smaller than overall limit, do nothing\n",
        "    if len(text) <= MAX_CONTEXT_CHARS:\n",
        "        return text\n",
        "\n",
        "    # split text into sentences\n",
        "    sentences, sentence_idxs = sentence_split(lang, text)\n",
        "    # print(text)\n",
        "    # print(sentences)\n",
        "\n",
        "    # find location position in the text\n",
        "    loc_regex = re.compile(r'\\[START\\].+?\\[END\\]')\n",
        "    loc = loc_regex.search(text)\n",
        "    loc_start, loc_end = loc.start(), loc.end()\n",
        "\n",
        "    # get idx of the sentence with the location\n",
        "    loc_sentence_id = get_location_sentence_id(loc_start, loc_end, sentence_idxs)\n",
        "\n",
        "    # print(loc.group(), loc_start, loc_end)\n",
        "    # print(sentences)\n",
        "    # print(sentence_idxs)\n",
        "\n",
        "    # find info on limited context (border sentence idxs)\n",
        "    context = find_context(\n",
        "        sentences, \n",
        "        loc_sentence_id, \n",
        "        (loc_start, loc_end), \n",
        "        sentence_idxs[loc_sentence_id][0]\n",
        "    )\n",
        "\n",
        "    # return cut context\n",
        "    return ' '.join(sentences[context['left']['idx']:context['right']['idx'] + 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-e7E5wtwbUGR"
      },
      "outputs": [],
      "source": [
        "# Get texts for locations and run GENRE inference.\n",
        "\n",
        "def get_text_parts(p_tag, text_only_for_loc=False):\n",
        "  \"\"\"\n",
        "  In <p> tag, separate raw text and XML elements. \n",
        "  Treat `note` tags as nested <p>.\n",
        "  if `text_only_for_loc` is True, write only raw text for loc tags.\n",
        "  \"\"\"\n",
        "  parts = [' ']\n",
        "  part_idxs = []\n",
        "\n",
        "  if p_tag.text:\n",
        "    parts.append(p_tag.text)\n",
        "  \n",
        "  for child in p_tag:\n",
        "    if child.tag == f'{{{NAMESPACE[\"ns\"]}}}note':\n",
        "      parts += get_text_parts(child, text_only_for_loc)\n",
        "      continue\n",
        "\n",
        "    if text_only_for_loc and child.tag == f'{{{NAMESPACE[\"ns\"]}}}loc':\n",
        "      parts.append(child.text)\n",
        "    else:\n",
        "      parts.append(child)\n",
        "\n",
        "    if child.tail:\n",
        "      parts.append(child.tail)\n",
        "  \n",
        "  return parts\n",
        "\n",
        "\n",
        "def create_text_for_location(loc_idx, selected_parts):\n",
        "    \"\"\"\n",
        "    Given text parts, create representation \n",
        "    for GENRE inference for one location\n",
        "    \"\"\"\n",
        "    final_parts = []\n",
        "    for i, part in enumerate(selected_parts):\n",
        "      text = f'[START] {part} [END] ' if loc_idx == i else part\n",
        "      final_parts.append(text)\n",
        "    whole_text = SPACE_REGEX.sub(' ', ''.join(final_parts)).strip()\n",
        "    limited_context = cut_context(whole_text, lang)\n",
        "    return limited_context\n",
        "\n",
        "\n",
        "def compile_relevant_texts(parts, lang):\n",
        "    \"\"\"\n",
        "    From all parts of <p> tag (got from get_text_parts),\n",
        "    compile separate texts for GENRE inference \n",
        "    for each location in the text.\n",
        "    \"\"\"\n",
        "    selected_parts = []\n",
        "    loc_idxs = []\n",
        "\n",
        "    i = 0\n",
        "    for part in parts:\n",
        "      text = part if isinstance(part, str) else part.text\n",
        "  \n",
        "      if not isinstance(part, str):\n",
        "        # leave out <stage> tag\n",
        "        if part.tag == f'{{{NAMESPACE[\"ns\"]}}}stage':\n",
        "          continue\n",
        "\n",
        "        # save parts that contain target locations\n",
        "        elif part.tag == f'{{{NAMESPACE[\"ns\"]}}}loc':\n",
        "          loc_idxs.append(i)\n",
        "\n",
        "      selected_parts.append(text)\n",
        "      i += 1\n",
        "    \n",
        "    # create separate texts for each location\n",
        "    final_texts = []\n",
        "    for loc_idx in loc_idxs:\n",
        "      loc_text = create_text_for_location(loc_idx, selected_parts)\n",
        "      final_texts.append(loc_text)\n",
        "\n",
        "    return final_texts\n",
        "\n",
        "\n",
        "def get_location_texts_for_utterance(prev_p_tag, p_tags, lang):\n",
        "  \"\"\"\n",
        "  Parse locations in all <p> tags inside one speaker's utterance\n",
        "  and create texts for inference for each location.\n",
        "  Include <p> tag from previous utterance \n",
        "  for more informative left context.\n",
        "  \"\"\"\n",
        "  utterance_loc_texts = []\n",
        "  \n",
        "  prev_parts = []\n",
        "  if prev_p_tag is not None:\n",
        "    prev_parts = get_text_parts(prev_p_tag, text_only_for_loc=True)\n",
        "\n",
        "  # one flat list of parts for the whole utterance (multiple <p> tags)\n",
        "  all_parts = (\n",
        "      prev_parts \n",
        "      + [' '] \n",
        "      + [part for p_tag in p_tags for part in get_text_parts(p_tag)] \n",
        "  )\n",
        "  utterance_loc_texts = compile_relevant_texts(all_parts, lang)\n",
        "\n",
        "  return utterance_loc_texts\n",
        "\n",
        "\n",
        "def get_location_texts_for_play(lang, tree):\n",
        "  \"\"\"\n",
        "  Given XML of a play, find all utterances with locations,\n",
        "  create a text for GENRE inference for each location.\n",
        "  \"\"\"\n",
        "  linking_texts = []\n",
        "  \n",
        "  utterances = tree.xpath(UTTERANCE_XPATH, namespaces=NAMESPACE)\n",
        "  print('N utterances', len(utterances))\n",
        "\n",
        "  prev_p = None\n",
        "  for utterance in utterances:\n",
        "    p_tags = utterance.xpath(TEXT_XPATH, namespaces=NAMESPACE)\n",
        "    if not p_tags:\n",
        "      prev_p = None\n",
        "      continue\n",
        "\n",
        "    if utterance.xpath(LOC_XPATH, namespaces=NAMESPACE):\n",
        "      linking_texts += get_location_texts_for_utterance(prev_p, p_tags, lang)\n",
        "\n",
        "    prev_p = p_tags[-1]\n",
        "  \n",
        "  return linking_texts\n",
        "\n",
        "\n",
        "def link_locations_in_play(linking_texts):\n",
        "  \"\"\"\n",
        "  Run GENRE inference, return texts with linked locations.\n",
        "  \"\"\"\n",
        "  # run inference\n",
        "  linked_locations = GENRE_MODEL.sample(\n",
        "    linking_texts,\n",
        "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
        "        e for e in TRIE.get(sent.tolist()) \n",
        "        if e < len(GENRE_MODEL.task.target_dictionary)\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # prepare links for json serialization\n",
        "  for loc_group in linked_locations:\n",
        "    for loc in loc_group:\n",
        "      loc['score'] = float(loc['score'])\n",
        "\n",
        "  return [\n",
        "      [text, locs] \n",
        "      for text, locs in zip(linking_texts, linked_locations)\n",
        "  ]\n",
        "\n",
        "\n",
        "def process_play(lang, xml_path):\n",
        "  \"\"\"\n",
        "  The main function.\n",
        "  Get texts for each location in the play\n",
        "  and run GENRE on them.\n",
        "  \"\"\"\n",
        "  with open(xml_path) as f:\n",
        "    tree = etree.parse(f)\n",
        "\n",
        "  linking_texts = get_location_texts_for_play(lang, tree)\n",
        "  linking_results = link_locations_in_play(linking_texts)\n",
        "  return linking_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "s5GEAwPxbpt0"
      },
      "outputs": [],
      "source": [
        "def parse_title(raw_title):\n",
        "    \"\"\"Divide GENRE output result into title and language.\"\"\"\n",
        "    parts = TITLE_REGEX.search(raw_title)\n",
        "    title, lang = parts.group(1), parts.group(2)\n",
        "    return title, lang\n",
        "\n",
        "\n",
        "def get_unique_titles(location_candidates):\n",
        "    \"\"\"\n",
        "    From GENRE output for all contexts, compile \n",
        "    lists of unique wiki titles for relevant languages.\n",
        "    \"\"\"\n",
        "    unique_titles = defaultdict(set)\n",
        "    for play_locations in location_candidates.values():\n",
        "        for _, loc_group in play_locations:\n",
        "            for loc_title in loc_group:\n",
        "                title, lang = parse_title(loc_title['text'])\n",
        "                if lang in WIKILANGS:\n",
        "                    unique_titles[lang].add(title)\n",
        "    return unique_titles\n",
        "\n",
        "\n",
        "def run_request(lang, titles):\n",
        "  \"\"\"Request Wikipedia API by Wikipedia titles.\"\"\"\n",
        "  r = requests.get(\n",
        "      REQUEST_URL % lang,\n",
        "      params={\n",
        "          'action': 'query',\n",
        "          'prop': 'pageprops|info',\n",
        "          'ppprop': 'wikibase_item',\n",
        "          'inprop': 'url',\n",
        "          'redirects': 1,\n",
        "          'titles': titles,\n",
        "          'format': 'json'\n",
        "      },\n",
        "      headers=USER_AGENT\n",
        "  ).json()\n",
        "  return r\n",
        "\n",
        "\n",
        "def query_wikipedia_unique(titles):\n",
        "    \"\"\"\n",
        "    Run Wikipedia API requests for all unique titles returned by GENRE. \n",
        "    Retrieve Wikidata ID and Wikipedia URL.\n",
        "    \"\"\"\n",
        "    links_by_title = {}\n",
        "    for lang, title_set in titles.items():\n",
        "      title_list = list(title_set)\n",
        "\n",
        "      # query by multiple titles at once\n",
        "      for i in range(0, len(title_list), N_TITLES_PER_REQUEST):\n",
        "          curr_titles = title_list[i:i + N_TITLES_PER_REQUEST]\n",
        "          titles = '|'.join(curr_titles)\n",
        "\n",
        "          r = run_request(lang, titles)\n",
        "          for k, v in r['query']['pages'].items():\n",
        "              if not k.startswith('-'):\n",
        "                links_by_title[v['title']] = {\n",
        "                    'wikidata_id': v['pageprops']['wikibase_item'],\n",
        "                    'url': v['fullurl']\n",
        "                }\n",
        "\n",
        "          sleep(1)\n",
        "\n",
        "    return links_by_title\n",
        "\n",
        "\n",
        "def reformat_location_candidates(location_candidates, links_by_title):\n",
        "    \"\"\"\n",
        "    Filter and reformat GENRE output.\n",
        "    For each valid prediction add Wikidata ID and Wikipedia URL.\n",
        "    \"\"\"\n",
        "    for play, play_locations in location_candidates.items():\n",
        "      new_play_locations = []\n",
        "\n",
        "      for text, loc_group in play_locations:\n",
        "          new_loc_group = []\n",
        "  \n",
        "          for loc in loc_group:\n",
        "              title, lang = parse_title(loc['text'])\n",
        "              is_confident = loc['score'] > GENRE_THRESHOLD\n",
        "              corr_lang = lang in WIKILANGS\n",
        "              exists = title in links_by_title\n",
        "\n",
        "              if corr_lang and is_confident and exists:\n",
        "                  link = links_by_title[title]\n",
        "                  new_loc_group.append(\n",
        "                      (link['wikidata_id'], loc['score'], link['url'])\n",
        "                  )\n",
        "\n",
        "          new_play_locations.append({\n",
        "              'text': text,\n",
        "              'scores': new_loc_group\n",
        "          })\n",
        "        \n",
        "      location_candidates[play] = new_play_locations\n",
        "\n",
        "    return location_candidates\n",
        "\n",
        "\n",
        "def get_wikidata_info(locations):\n",
        "    \"\"\"\n",
        "    Get wiki data for all unique GENRE titles;\n",
        "    filter and reformat all GENRE predictions.\n",
        "    \"\"\"\n",
        "    unique_titles = get_unique_titles(locations)\n",
        "    links_by_title = query_wikipedia_unique(unique_titles)\n",
        "    locations = reformat_location_candidates(locations, links_by_title)\n",
        "    return locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruREA-QmZA1J",
        "outputId": "bad7db72-c393-4a68-c662-f5ba26512cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valle-luces.xml\n",
            "N utterances 1174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fairseq/search.py:205: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:659: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69\n",
            "valle-romance.xml\n",
            "N utterances 950\n",
            "14\n",
            "munoz-ortiz.xml\n",
            "N utterances 1218\n",
            "50\n",
            "galdos-electra.xml\n",
            "N utterances 1588\n",
            "20\n",
            "valle-cara.xml\n",
            "N utterances 1186\n",
            "25\n",
            "valera-atahualpa.xml\n",
            "N utterances 371\n",
            "48\n",
            "galdos-perfecta.xml\n",
            "N utterances 1165\n",
            "27\n",
            "galdos-casandra.xml\n",
            "N utterances 997\n",
            "10\n",
            "munoz-refugio.xml\n",
            "N utterances 1361\n",
            "82\n",
            "echegaray-arrastrarse.xml\n",
            "N utterances 1599\n",
            "43\n",
            "ostrovsky-beshenye-dengi.xml\n",
            "N utterances 1214\n",
            "54\n",
            "bulgakov-zojkina-kvartira.xml\n",
            "N utterances 1406\n",
            "104\n",
            "chekhov-tri-sestry.xml\n",
            "N utterances 758\n",
            "67\n",
            "ostrovsky-groza.xml\n",
            "N utterances 784\n",
            "20\n",
            "ostrovsky-bespridannitsa.xml\n",
            "N utterances 1242\n",
            "52\n",
            "turgenev-holostjak.xml\n",
            "N utterances 883\n",
            "33\n",
            "bulgakov-beg.xml\n",
            "N utterances 821\n",
            "150\n",
            "gogol-revizor.xml\n",
            "N utterances 927\n",
            "45\n",
            "chekhov-vishnevyi-sad.xml\n",
            "N utterances 634\n",
            "36\n",
            "petrov-ostrov-mira.xml\n",
            "N utterances 636\n",
            "133\n"
          ]
        }
      ],
      "source": [
        "langs = os.listdir('final')\n",
        "locations = {}\n",
        "\n",
        "for lang in langs:\n",
        "  if lang.startswith('.'):\n",
        "    continue\n",
        "\n",
        "  corpus_dir = os.path.join('final', lang)\n",
        "  for playname in os.listdir(corpus_dir):\n",
        "    if not playname.endswith('.xml'):\n",
        "    # if not playname == 'petrov-ostrov-mira.xml':\n",
        "      continue\n",
        "\n",
        "    print(playname)\n",
        "    xml_path = os.path.join(corpus_dir, playname)\n",
        "    play_locations = process_play(lang, xml_path)\n",
        "    locations[playname[:-4]] = play_locations\n",
        "    print(len(play_locations))\n",
        "\n",
        "locations = get_wikidata_info(locations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "fphTuNr6g-YF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('location_links_genre.json', 'w') as f:\n",
        "  json.dump(locations, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YDXKoWfEzwkx"
      },
      "outputs": [],
      "source": [
        "!cp location_links_genre.json /content/drive/MyDrive/SWT/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "26y5rtEHgeVS"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBfWzapr0Bdw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hpyUXp5a86Oo"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}