{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpyUXp5a86Oo"
      },
      "source": [
        "## Installation and downloads\n",
        "original debugging code is [here](https://colab.research.google.com/drive/1Lx9pIxX0JYOGFG0Aoe39qW4N5vmOV74G?usp=sharing).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed0HCRYTzU1x"
      },
      "outputs": [],
      "source": [
        "! wget https://dl.fbaipublicfiles.com/GENRE/fairseq_multilingual_entity_disambiguation.tar.gz\n",
        "! tar -xvf fairseq_multilingual_entity_disambiguation.tar.gz\n",
        "\n",
        "! wget http://dl.fbaipublicfiles.com/GENRE/titles_lang_all105_marisa_trie_with_redirect.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgImLyR_x0hr"
      },
      "outputs": [],
      "source": [
        "# remove locally installed `examples` package to avoid import error\n",
        "! rm -rf /usr/local/lib/python3.7/dist-packages/examples\n",
        "\n",
        "! git clone --branch fixing_prefix_allowed_tokens_fn https://github.com/nicola-decao/fairseq\n",
        "\n",
        "# remove the bugged lines in this fairseq version\n",
        "! sed -i -e '26,27d' /content/fairseq/fairseq/registry.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPqPsJaZYdZ1"
      },
      "outputs": [],
      "source": [
        "! cd fairseq && pip install  ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb--qs5OAx2n"
      },
      "outputs": [],
      "source": [
        "# further path fixes\n",
        "\n",
        "! mkdir -p examples_dir\n",
        "! cp -r /content/fairseq/examples/ /content/examples_dir\n",
        "! ls /content/examples_dir\n",
        "\n",
        "! cp -r /content/fairseq/fairseq/models/speech_to_text/modules /usr/local/lib/python3.7/dist-packages/fairseq/models/speech_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPtRnBalzLGW"
      },
      "outputs": [],
      "source": [
        "!rm -rf GENRE\n",
        "!git clone https://github.com/facebookresearch/GENRE.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3sSRRHe8lWD"
      },
      "outputs": [],
      "source": [
        "! cd GENRE && pip install ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSkPf1xQ0p0k"
      },
      "outputs": [],
      "source": [
        "! pip install sentencepiece marisa_trie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUw14XCQIlRE"
      },
      "outputs": [],
      "source": [
        "# ! pip install spacy\n",
        "! python -m spacy download es_core_news_sm\n",
        "! python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbcckrQ9NKb"
      },
      "source": [
        "# Parse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJ5Hbgb3LDQN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CnFypT0oa0RO"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/SWT/final/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cdEUlzWbBU-O"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/GENRE/genre')\n",
        "sys.path.append('/content/examples_dir')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8HX-jWO6zyOF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from time import sleep\n",
        "\n",
        "import spacy\n",
        "import requests\n",
        "from lxml import etree\n",
        "\n",
        "from fairseq_model import mGENRE\n",
        "from genre.trie import Trie, MarisaTrie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Fn4QSkyacGUM"
      },
      "outputs": [],
      "source": [
        "NAMESPACE = {'ns': 'http://www.tei-c.org/ns/1.0'}\n",
        "UTTERANCE_XPATH = '//ns:sp'\n",
        "TEXT_XPATH = './ns:p'\n",
        "LOC_XPATH = './/ns:loc'\n",
        "\n",
        "SPACE_REGEX = re.compile(r'\\s+')\n",
        "\n",
        "MAX_CONTEXT_CHARS = 1000  # context limit from one side of location\n",
        "\n",
        "REQUEST_URL = 'https://%s.wikipedia.org/w/api.php'\n",
        "USER_AGENT = {'User-Agent': 'Location extractor (e.garanina@student.rug.nl)'}\n",
        "N_TITLES_PER_REQUEST = 50\n",
        "\n",
        "TITLE_REGEX = re.compile(r'^(.+?) >> (.+)$')\n",
        "GENRE_THRESHOLD = -0.65\n",
        "\n",
        "WIKILANGS = {'ru', 'es'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LrPfYusC2bxH"
      },
      "outputs": [],
      "source": [
        "GENRE_MODEL = mGENRE.from_pretrained(\"fairseq_multilingual_entity_disambiguation\").eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HwBmFYE31JxO"
      },
      "outputs": [],
      "source": [
        "# memory efficient but slower prefix tree (trie) -- it is implemented with `marisa_trie`\n",
        "with open(\"titles_lang_all105_marisa_trie_with_redirect.pkl\", \"rb\") as f:\n",
        "    TRIE = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RkoLd6mVI8Sx"
      },
      "outputs": [],
      "source": [
        "SPACY_OBJS = {\n",
        "    'span': spacy.load(\"es_core_news_sm\"),\n",
        "    'rus': spacy.load(\"ru_core_news_sm\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VZvvqFAOJwQu"
      },
      "outputs": [],
      "source": [
        "# Cut context of the location so that it fits into the model.\n",
        "\n",
        "def sentence_split(lang, text):\n",
        "    \"\"\"\n",
        "    Split text into sentences.\n",
        "    Return sentences and their indices in the text.\n",
        "    \"\"\"\n",
        "    # workaround to avoid splitting by [START] or [END]\n",
        "    clear_text = text.replace('[START] ', 'a' * 8).replace(' [END]', 'a' * 6)\n",
        "\n",
        "    nlp = SPACY_OBJS[lang]\n",
        "    doc = nlp(clear_text)\n",
        "    assert doc.has_annotation(\"SENT_START\")\n",
        "  \n",
        "    sentence_idxs = [(s.start_char, s.end_char) for s in doc.sents]\n",
        "    sentences = [text[s:e] for s, e in sentence_idxs]\n",
        "  \n",
        "    return sentences, sentence_idxs\n",
        "\n",
        "\n",
        "def get_location_sentence_id(loc_start, loc_end, sentence_idxs):\n",
        "    \"\"\"\n",
        "    Get sentence idx of the parsed location\n",
        "    based on indices of location and sentences in the text.\n",
        "    \"\"\"\n",
        "    loc_sentence_id = 0\n",
        "    n_sentences = len(sentence_idxs)\n",
        "\n",
        "    # iterate until start of location is in the current sentence\n",
        "    while loc_start >= sentence_idxs[loc_sentence_id][1]:\n",
        "        loc_sentence_id += 1\n",
        "\n",
        "    # print(loc_start, loc_end, sentence_idxs)\n",
        "    # end of location must be in the same sentence\n",
        "    assert loc_end < sentence_idxs[loc_sentence_id][1]\n",
        "    return loc_sentence_id\n",
        "\n",
        "\n",
        "def find_context(sentences, loc_sentence_id, loc_span, loc_sentence_start):\n",
        "    \"\"\"\n",
        "    Limit context for locations \n",
        "    (due to sequence length restriction in the model).\n",
        "    Take sentences from left and right until \n",
        "    the context length reaches MAX_CONTEXT_CHARS.\n",
        "    \"\"\"\n",
        "    context = {\n",
        "        'left': {\n",
        "            'offset': 0,  # absolute diff between current sentence idx and loc_sentence_id\n",
        "            'n_chars': 0,  # number of characters in already taken sentences\n",
        "            'increment': -1,  # \"direction\" of incrementing the indices from loc_sentence_id\n",
        "            'start': loc_span[0] - loc_sentence_start  # start idx of location inside loc_sentence_id\n",
        "        },\n",
        "        'right': {\n",
        "            'offset': 0,\n",
        "            'n_chars': 0,\n",
        "            'increment': 1,\n",
        "            'start': loc_span[1] - loc_sentence_start\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # consider right and left context separately\n",
        "    for key, info in context.items():\n",
        "        idx = loc_sentence_id\n",
        "        stable_idx = loc_sentence_id\n",
        "\n",
        "        # take new sentences until n_chars exceeds limit\n",
        "        while info['n_chars'] < MAX_CONTEXT_CHARS:\n",
        "            stable_idx = idx  # previous idx which did not exceed the limit\n",
        "\n",
        "            # if we consider sentence with loc (at first step), \n",
        "            # add to n_chars n_symbols before / after the location\n",
        "            if info['offset'] == 0:\n",
        "                info['n_chars'] += len(sentences[loc_sentence_id][:info['start']])\n",
        "            else:\n",
        "                # new index ty try\n",
        "                idx = loc_sentence_id + (info['offset'] * info['increment'])\n",
        "\n",
        "                # if new idx is out of range, end the loop\n",
        "                if idx < 0 or idx == len(sentences):\n",
        "                  break\n",
        "\n",
        "                info['n_chars'] += len(sentences[idx])\n",
        "\n",
        "            # increasing the diff (i.e. going one sent further)\n",
        "            info['offset'] += 1\n",
        "\n",
        "        # if we're out of the loop, \n",
        "        # info['idx'] is out of the range or limit of n_chars is exceeded, \n",
        "        # so take idx from previous iteration, which passed\n",
        "        info['idx'] = stable_idx\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def cut_context(text, lang):\n",
        "    \"\"\" \n",
        "    Cut sentences from left and right context of location.\n",
        "    Split text into sentences with spacy, \n",
        "    cut sentences which do not fit into MAX_CONTEXT_CHARS limit.\n",
        "    \"\"\"\n",
        "    # if the whole text is much smaller than overall limit, do nothing\n",
        "    if len(text) <= MAX_CONTEXT_CHARS:\n",
        "        return text\n",
        "\n",
        "    # split text into sentences\n",
        "    sentences, sentence_idxs = sentence_split(lang, text)\n",
        "    # print(text)\n",
        "    # print(sentences)\n",
        "\n",
        "    # find location position in the text\n",
        "    loc_regex = re.compile(r'\\[START\\].+?\\[END\\]')\n",
        "    loc = loc_regex.search(text)\n",
        "    loc_start, loc_end = loc.start(), loc.end()\n",
        "\n",
        "    # get idx of the sentence with the location\n",
        "    loc_sentence_id = get_location_sentence_id(loc_start, loc_end, sentence_idxs)\n",
        "\n",
        "    # print(loc.group(), loc_start, loc_end)\n",
        "    # print(sentences)\n",
        "    # print(sentence_idxs)\n",
        "\n",
        "    # find info on limited context (border sentence idxs)\n",
        "    context = find_context(\n",
        "        sentences, \n",
        "        loc_sentence_id, \n",
        "        (loc_start, loc_end), \n",
        "        sentence_idxs[loc_sentence_id][0]\n",
        "    )\n",
        "\n",
        "    # return cut context\n",
        "    return ' '.join(sentences[context['left']['idx']:context['right']['idx'] + 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "-e7E5wtwbUGR"
      },
      "outputs": [],
      "source": [
        "# Get texts for locations and run GENRE inference.\n",
        "\n",
        "def get_text_parts(p_tag, text_only_for_loc=False):\n",
        "  \"\"\"\n",
        "  In <p> tag, separate raw text and XML elements. \n",
        "  Treat `note` tags as nested <p>.\n",
        "  if `text_only_for_loc` is True, write only raw text for loc tags.\n",
        "  \"\"\"\n",
        "  parts = [' ']\n",
        "  part_idxs = []\n",
        "\n",
        "  if p_tag.text:\n",
        "    parts.append(p_tag.text)\n",
        "  \n",
        "  for child in p_tag:\n",
        "    if child.tag == f'{{{NAMESPACE[\"ns\"]}}}note':\n",
        "      parts += get_text_parts(child, text_only_for_loc)\n",
        "      continue\n",
        "\n",
        "    if text_only_for_loc and child.tag == f'{{{NAMESPACE[\"ns\"]}}}loc':\n",
        "      parts.append(child.text)\n",
        "    else:\n",
        "      parts.append(child)\n",
        "\n",
        "    if child.tail:\n",
        "      parts.append(child.tail)\n",
        "  \n",
        "  return parts\n",
        "\n",
        "\n",
        "def create_text_for_location(loc_idx, selected_parts):\n",
        "    \"\"\"\n",
        "    Given text parts, create representation \n",
        "    for GENRE inference for one location\n",
        "    \"\"\"\n",
        "    final_parts = []\n",
        "    for i, part in enumerate(selected_parts):\n",
        "      text = f'[START] {part} [END] ' if loc_idx == i else part\n",
        "      final_parts.append(text)\n",
        "    whole_text = SPACE_REGEX.sub(' ', ''.join(final_parts)).strip()\n",
        "    limited_context = cut_context(whole_text, lang)\n",
        "    return limited_context\n",
        "\n",
        "\n",
        "def compile_relevant_texts(parts, lang):\n",
        "    \"\"\"\n",
        "    From all parts of <p> tag (got from get_text_parts),\n",
        "    compile separate texts for GENRE inference \n",
        "    for each location in the text.\n",
        "    \"\"\"\n",
        "    selected_parts = []\n",
        "    loc_idxs = []\n",
        "\n",
        "    i = 0\n",
        "    for part in parts:\n",
        "      text = part if isinstance(part, str) else part.text\n",
        "  \n",
        "      if not isinstance(part, str):\n",
        "        # leave out <stage> tag\n",
        "        if part.tag == f'{{{NAMESPACE[\"ns\"]}}}stage':\n",
        "          continue\n",
        "\n",
        "        # save parts that contain target locations\n",
        "        elif part.tag == f'{{{NAMESPACE[\"ns\"]}}}loc':\n",
        "          loc_idxs.append(i)\n",
        "\n",
        "      selected_parts.append(text)\n",
        "      i += 1\n",
        "    \n",
        "    # create separate texts for each location\n",
        "    final_texts = []\n",
        "    for loc_idx in loc_idxs:\n",
        "      loc_text = create_text_for_location(loc_idx, selected_parts)\n",
        "      final_texts.append(loc_text)\n",
        "\n",
        "    return final_texts\n",
        "\n",
        "\n",
        "def get_location_texts_for_utterance(prev_p_tag, p_tags, lang):\n",
        "  \"\"\"\n",
        "  Parse locations in all <p> tags inside one speaker's utterance\n",
        "  and create texts for inference for each location.\n",
        "  Include <p> tag from previous utterance \n",
        "  for more informative left context.\n",
        "  \"\"\"\n",
        "  utterance_loc_texts = []\n",
        "  \n",
        "  prev_parts = []\n",
        "  if prev_p_tag is not None:\n",
        "    prev_parts = get_text_parts(prev_p_tag, text_only_for_loc=True)\n",
        "\n",
        "  # one flat list of parts for the whole utterance (multiple <p> tags)\n",
        "  all_parts = (\n",
        "      prev_parts \n",
        "      + [' '] \n",
        "      + [part for p_tag in p_tags for part in get_text_parts(p_tag)] \n",
        "  )\n",
        "  utterance_loc_texts = compile_relevant_texts(all_parts, lang)\n",
        "\n",
        "  return utterance_loc_texts\n",
        "\n",
        "\n",
        "def get_location_texts_for_play(lang, tree):\n",
        "  \"\"\"\n",
        "  Given XML of a play, find all utterances with locations,\n",
        "  create a text for GENRE inference for each location.\n",
        "  \"\"\"\n",
        "  linking_texts = []\n",
        "  \n",
        "  utterances = tree.xpath(UTTERANCE_XPATH, namespaces=NAMESPACE)\n",
        "  print('N utterances', len(utterances))\n",
        "\n",
        "  prev_p = None\n",
        "  for utterance in utterances:\n",
        "    p_tags = utterance.xpath(TEXT_XPATH, namespaces=NAMESPACE)\n",
        "    if not p_tags:\n",
        "      prev_p = None\n",
        "      continue\n",
        "\n",
        "    if utterance.xpath(LOC_XPATH, namespaces=NAMESPACE):\n",
        "      linking_texts += get_location_texts_for_utterance(prev_p, p_tags, lang)\n",
        "\n",
        "    prev_p = p_tags[-1]\n",
        "  \n",
        "  return linking_texts\n",
        "\n",
        "\n",
        "def link_locations_in_play(linking_texts):\n",
        "  \"\"\"\n",
        "  Run GENRE inference, return texts with linked locations.\n",
        "  \"\"\"\n",
        "  # run inference\n",
        "  linked_locations = GENRE_MODEL.sample(\n",
        "    linking_texts,\n",
        "    prefix_allowed_tokens_fn=lambda batch_id, sent: [\n",
        "        e for e in TRIE.get(sent.tolist()) \n",
        "        if e < len(GENRE_MODEL.task.target_dictionary)\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  # prepare links for json serialization\n",
        "  for loc_group in linked_locations:\n",
        "    for loc in loc_group:\n",
        "      loc['score'] = float(loc['score'])\n",
        "\n",
        "  return [\n",
        "      [text, locs] \n",
        "      for text, locs in zip(linking_texts, linked_locations)\n",
        "  ]\n",
        "\n",
        "\n",
        "def process_play(lang, xml_path):\n",
        "  \"\"\"\n",
        "  The main function.\n",
        "  Get texts for each location in the play\n",
        "  and run GENRE on them.\n",
        "  \"\"\"\n",
        "  with open(xml_path) as f:\n",
        "    tree = etree.parse(f)\n",
        "\n",
        "  linking_texts = get_location_texts_for_play(lang, tree)\n",
        "  linking_results = link_locations_in_play(linking_texts)\n",
        "  return linking_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "s5GEAwPxbpt0"
      },
      "outputs": [],
      "source": [
        "def parse_title(raw_title):\n",
        "    \"\"\"Divide GENRE output result into title and language.\"\"\"\n",
        "    parts = TITLE_REGEX.search(raw_title)\n",
        "    title, lang = parts.group(1), parts.group(2)\n",
        "    return title, lang\n",
        "\n",
        "\n",
        "def get_unique_titles(location_candidates):\n",
        "    \"\"\"\n",
        "    From GENRE output for all contexts, compile \n",
        "    lists of unique wiki titles for relevant languages.\n",
        "    \"\"\"\n",
        "    unique_titles = defaultdict(set)\n",
        "    for play_locations in location_candidates.values():\n",
        "        for _, loc_group in play_locations:\n",
        "            for loc_title in loc_group:\n",
        "                title, lang = parse_title(loc_title['text'])\n",
        "                if lang in WIKILANGS:\n",
        "                    unique_titles[lang].add(title)\n",
        "    return unique_titles\n",
        "\n",
        "\n",
        "def run_request(lang, titles):\n",
        "  \"\"\"Request Wikipedia API by Wikipedia titles.\"\"\"\n",
        "  r = requests.get(\n",
        "      REQUEST_URL % lang,\n",
        "      params={\n",
        "          'action': 'query',\n",
        "          'prop': 'pageprops|info',\n",
        "          'ppprop': 'wikibase_item',\n",
        "          'inprop': 'url',\n",
        "          'redirects': 1,\n",
        "          'titles': titles,\n",
        "          'format': 'json'\n",
        "      },\n",
        "      headers=USER_AGENT\n",
        "  ).json()\n",
        "  return r\n",
        "\n",
        "\n",
        "def query_wikipedia_unique(titles):\n",
        "    \"\"\"\n",
        "    Run Wikipedia API requests for all unique titles returned by GENRE. \n",
        "    Retrieve Wikidata ID and Wikipedia URL.\n",
        "    \"\"\"\n",
        "    links_by_title = {}\n",
        "    for lang, title_set in titles.items():\n",
        "      title_list = list(title_set)\n",
        "\n",
        "      # query by multiple titles at once\n",
        "      for i in range(0, len(title_list), N_TITLES_PER_REQUEST):\n",
        "          curr_titles = title_list[i:i + N_TITLES_PER_REQUEST]\n",
        "          titles = '|'.join(curr_titles)\n",
        "\n",
        "          r = run_request(lang, titles)\n",
        "          for k, v in r['query']['pages'].items():\n",
        "              if not k.startswith('-'):\n",
        "                links_by_title[v['title']] = {\n",
        "                    'wikidata_id': v['pageprops']['wikibase_item'],\n",
        "                    'url': v['fullurl']\n",
        "                }\n",
        "\n",
        "          sleep(1)\n",
        "\n",
        "    return links_by_title\n",
        "\n",
        "\n",
        "def reformat_location_candidates(location_candidates, links_by_title):\n",
        "    \"\"\"\n",
        "    Filter and reformat GENRE output.\n",
        "    For each valid prediction add Wikidata ID and Wikipedia URL.\n",
        "    \"\"\"\n",
        "    for play, play_locations in location_candidates.items():\n",
        "      new_play_locations = []\n",
        "\n",
        "      for text, loc_group in play_locations:\n",
        "          new_loc_group = []\n",
        "  \n",
        "          for loc in loc_group:\n",
        "              title, lang = parse_title(loc['text'])\n",
        "              is_confident = loc['score'] > GENRE_THRESHOLD\n",
        "              corr_lang = lang in WIKILANGS\n",
        "              exists = title in links_by_title\n",
        "\n",
        "              if corr_lang and is_confident and exists:\n",
        "                  link = links_by_title[title]\n",
        "                  new_loc_group.append(\n",
        "                      (link['wikidata_id'], loc['score'], link['url'])\n",
        "                  )\n",
        "\n",
        "          new_play_locations.append({\n",
        "              'text': text,\n",
        "              'scores': new_loc_group\n",
        "          })\n",
        "        \n",
        "      location_candidates[play] = new_play_locations\n",
        "\n",
        "    return location_candidates\n",
        "\n",
        "\n",
        "def get_wikidata_info(locations):\n",
        "    \"\"\"\n",
        "    Get wiki data for all unique GENRE titles;\n",
        "    filter and reformat all GENRE predictions.\n",
        "    \"\"\"\n",
        "    unique_titles = get_unique_titles(locations)\n",
        "    links_by_title = query_wikipedia_unique(unique_titles)\n",
        "    locations = reformat_location_candidates(locations, links_by_title)\n",
        "    return locations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruREA-QmZA1J",
        "outputId": "bad7db72-c393-4a68-c662-f5ba26512cdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valle-luces.xml\n",
            "N utterances 1174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fairseq/search.py:205: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:659: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n",
            "valle-romance.xml\n",
            "N utterances 950\n",
            "14\n",
            "munoz-ortiz.xml\n",
            "N utterances 1218\n",
            "50\n",
            "galdos-electra.xml\n",
            "N utterances 1588\n",
            "20\n",
            "valle-cara.xml\n",
            "N utterances 1186\n",
            "25\n",
            "valera-atahualpa.xml\n",
            "N utterances 371\n",
            "50\n",
            "galdos-perfecta.xml\n",
            "N utterances 1165\n",
            "28\n",
            "galdos-casandra.xml\n",
            "N utterances 997\n",
            "10\n",
            "munoz-refugio.xml\n",
            "N utterances 1361\n",
            "82\n",
            "echegaray-arrastrarse.xml\n",
            "N utterances 1599\n",
            "43\n",
            "ostrovsky-beshenye-dengi.xml\n",
            "N utterances 1214\n",
            "54\n",
            "bulgakov-zojkina-kvartira.xml\n",
            "N utterances 1406\n",
            "104\n",
            "chekhov-tri-sestry.xml\n",
            "N utterances 758\n",
            "67\n",
            "ostrovsky-groza.xml\n",
            "N utterances 784\n",
            "20\n",
            "ostrovsky-bespridannitsa.xml\n",
            "N utterances 1242\n",
            "52\n",
            "turgenev-holostjak.xml\n",
            "N utterances 883\n",
            "33\n",
            "bulgakov-beg.xml\n",
            "N utterances 821\n",
            "150\n",
            "gogol-revizor.xml\n",
            "N utterances 927\n",
            "45\n",
            "chekhov-vishnevyi-sad.xml\n",
            "N utterances 634\n",
            "36\n",
            "petrov-ostrov-mira.xml\n",
            "N utterances 636\n",
            "133\n"
          ]
        }
      ],
      "source": [
        "langs = os.listdir('final')\n",
        "locations = {}\n",
        "\n",
        "for lang in langs:\n",
        "  if lang.startswith('.'):\n",
        "    continue\n",
        "\n",
        "  corpus_dir = os.path.join('final', lang)\n",
        "  for playname in os.listdir(corpus_dir):\n",
        "    if not playname.endswith('.xml'):\n",
        "    # if not playname == 'petrov-ostrov-mira.xml':\n",
        "      continue\n",
        "\n",
        "    print(playname)\n",
        "    xml_path = os.path.join(corpus_dir, playname)\n",
        "    play_locations = process_play(lang, xml_path)\n",
        "    locations[playname[:-4]] = play_locations\n",
        "    print(len(play_locations))\n",
        "\n",
        "locations = get_wikidata_info(locations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "fphTuNr6g-YF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('genre_ranking.json', 'w') as f:\n",
        "  json.dump(locations, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "YDXKoWfEzwkx"
      },
      "outputs": [],
      "source": [
        "!cp genre_ranking.json /content/drive/MyDrive/SWT/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "26y5rtEHgeVS"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hpyUXp5a86Oo"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
